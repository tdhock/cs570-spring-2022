Neural network regularization 

In previous projects we have mainly used the number of
iterations/epochs of gradient descent as the regularization
parameter. In this project the goal is to demonstrate other techniques
for regularizing neural networks. For this project you need to use
torch.

** Overview

The goal is to do computational experiments that demonstrate a
parameter that can be tuned to regularize a neural network. The
experiment should result in a plot of subtrain/validation loss (on the
y axis) as a function of a regularization parameter (on the x
axis). It is your choice about which regularization parameter to
investigate, and you can get extra credit if you investigate a
regularization parameter that is described in the book, but we have
not discussed in class/[[https://www.youtube.com/playlist?list=PLwc48KSH3D1MvTf_JOI00_eIPcoeYMM_o][screencasts]]. You may NOT choose to plot the
regularizing effect of early stopping (x axis = number of
iterations/epochs), because we have already done that in previous
projects. Here is a list of parameters mentioned in Chapter 7 that you
could investigate for extra credit:
- 7.1 Parameter norm penalties, X axis = degree of L2 / weight
  decay. 10 points extra credit.
- 7.5 noise robustness, X axis = degree of noise/perturbation. 10
  points extra credit.
- 7.12 dropout, X axis = probability of dropout. 10 points extra
  credit.
And here are the two regularization parameters that I demonstrated in
the screencasts linked above, and you can do something similar for
your project, for normal (not extra) credit:
- X axis = number of hidden units in a network with one hidden layer.
- X axis = number of hidden layers of a given size.

** Class: RegularizedMLP

This class should define a learner with fit and predict methods,
similar to what we did last week. Modify it so that you can specify
different regularization hyper-parameter values.

** Plotting loss vs regularization hyper-parameter

- Load the spam and zip data sets as usual from CSV.
- Scale the input matrix, same as in previous projects.
- Next divide each data set into 50% subtrain, 50% validation.
- Define a for loop over regularization hyper-parameter values, and
  fit a neural network for each, with a constant number of max epochs
  (say 1000).
- On the same plot, show the logistic loss as a function of the
  regularization hyper-parameter. X axis should be the regularization
  hyper-parameter you choose to study (for example, number of hidden
  units/layers), NOT the number of epochs. Use a different color for
  each set, e.g. subtrain=red, validation=blue. Draw a point to
  emphasize the minimum of the validation loss curve.
- Your plots should show the characteristic U shape of the validation
  loss curve, and monotonic subtrain loss curve. As the strength of
  regularization decreases, the subtrain loss should always decrease,
  whereas the validation loss should decrease up to a certain point,
  and then start increasing (overfitting). If your curves don't then
  you should try increasing the number of regularization
  hyper-parameters, and/or increasing the maximum model complexity
  (e.g. max=5 hidden units is too small to overfit, max=100 or 1000
  would be more likely to result in overfitting).
- Define a variable called best_hyper_parameter which is the
  regularization hyper-parameter which minimizes the validation loss
  (for a particular subtrain/validation split).
- Re-train the network on the entire train set (not just the subtrain
  set), using the corresponding best_hyper_parameter.
- Finally use the learned model to make predictions on the
  test set. What is the prediction accuracy? (percent correctly
  predicted labels in the test set) What is the prediction accuracy of
  the baseline model which predicts the most frequent class in the
  train labels?

** Test error experiment

- Create 
- Use 3-fold CV to create three train/test splits.
- In each split, use your 


Your final grade for this project will be computed by multiplying the
percentage from your [[file:group-evals.org][group evaluations]] with your group's total score
from the rubric above.

Your group should submit a PDF on BBLearn. 
- The first thing in the PDF should be your names and student ID's
  (e.g. th798) and a link to your source code in a public repo
  (e.g. github, there should be no code in your PDF report).
- The second thing in the PDF should be your group evaluation scores
  for yourself and your teammates.

Extra credit: 
- 10 points if your github repo includes a README.org (or README.md
  etc) file with a link to the source code of your script, and an
  explanation about how to install the necessary libraries, and run it
  on the data set.
- 10 points if you do 4-fold cross-validation instead of the single
  train/test split described above, and you make a plot of test
  accuracy for all models for each split/fold.
- 10 points if you compute and plot ROC curves for each (test fold,
  algorithm) combination. Make sure each algorithm is drawn in a
  different color, and there is a legend that the reader can use to
  read the figure. Example:

[[file:1-ROC.PNG]]
  
- 10 points if you compute area under the ROC curve (AUC) and include
  that as another evaluation metric (in a separate panel/plot) to
  compare the test accuracy of the algorithms.
