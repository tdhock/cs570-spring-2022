Implementing nearest neighbors classification and cross-validation

In this project your goal is to implement nearest neighbors and K-Fold
cross-validation from scratch, then compare your implementation to
the one you used last week in scikit-learn.

*** Class: MyKNN

This class should work just like sklearn.neighbors.KNeighborsClassifier.
- instantiate it with n_neighbors parameter.
- fit(X=train_features, y=train_labels) method should store data.
- predict(X=test_features) should compute a binary vector of predicted
  class labels. For each test data row, use numpy to compute the
  distances with all of the train data. Each distance is the square
  root of the sum of squared differences over all features (you don't
  have to take the square root though because it is a monotonic
  transformation, does not change the ordering of the
  observations). Sort the distances (numpy.argsort) to find the
  smallest n_neighbors values. The predicted class should be the most
  frequent label among the nearest n_neighbors.

*** Class: MyCV

This class should work just like sklearn.model_selection.GridSearchCV.
- instantiation: MyCV(estimator=MyKNN(), param_grid={'n_neighbors':range(20)}, cv=5)
- fit(X=train_features, y=train_labels) should compute the best number
  of neighbors using K-fold cross-validation, with the number of folds
  defined by the cv parameter. Begin by assigning random fold ID
  numbers to each observation. There should be for loops over the K
  subtrain/validation splits, and the parameters (n_neighbors) in
  param_grid. For each, use fit(*subtrain_data) and
  predict(validation_features) methods of the estimator, then save the
  validation accuracy. For each parameter (n_neighbors) in param_grid
  compute the mean test accuracy over the K splits. Finally, maximize
  the mean test accuracy to determine a best parameter (n_neighbors)
  to save in the best_params_ attribute.
- predict(X=test_features) should use best_params_ to set
  attributes/parameters (n_neighbors) of estimator, then run
  estimator.predict(X=test_features).

*** Experiments/application

- Use the same experimental setup as last week (with 3-fold CV
  train/test splits defined by KFold), but add your MyCV + MyKNN as a
  new algorithm to compare.
- Make sure to run experiments on both spam and zip data, and show a
  table of resulting test accuracy numbers, as well as a ggplot like
  last week.
- Does your implementation get similar test accuracy as scikit-learn?
  (it should!)
  
*** Extra credit

- Implement pipelines and scaling as well, similar to make_pipeline
  and StandardScaler from scikit-learn. Add your method to the test
  accuracy figure. Does your implementation get similar test accuracy
  values?
- Using the zip and/or spam data, implement a computational experiment
  that demonstrates adding noise features results in lower test
  accuracy, similar to the one shown in the class slides. Make a
  figure that shows test accuracy as a function of the number of noise
  features. Does test accuracy decrease with more noise features, as
  expected?

*** FAQ
