Backprop algorithm and training neural networks from scratch

Like last week, the goal is to implement a stochastic gradient descent
algorithm for a neural network from scratch. Last week there was a
monolithic compute_gradient method which contained all of the logic
for the backpropagation algorithm. This week we will break the
gradient computation into several different classes in order to
demonstrate how "automatic differentiation" systems work. The
advantage of such systems is "separation of concerns," which is a
common theme in computer science. In this instance we separate the
gradient computations into separate classes which represent operations
for computing predictions, so that the person who writes the fit()
method of the learning algorithm just needs to compose instances of
these classes, and does not need to understand the details of the
gradient computations.

** Class: InitialNode

Each instance of this class represents an initial node in the computation
graph (not computed as a function of other nodes), with attributes
- value: numpy array representing value computed at this node (created
  at instantiation during __init__, explained below).
- grad: numpy array representing gradient of the loss with respect to
  this node (created during backward method, explained below).
- You should be able to initialize via code like

#+BEGIN_SRC python
  import numpy as np
  feature_node = InitialNode(np.array([
      [1, 2],
      [2,3],
      [1,5]
  ]))
  label_node = InitialNode(np.array([[-1, 1, 2]]).T)
  weight_node = InitialNode(weight_mat)
#+END_SRC

** Class: Operation

This class represents a node in the computation graph which is
computed as a function of other nodes. This should be a virtual class,
which means it should not be instantiated directly, but instead it
should define methods that are used in subclasses.
- __init__ method should accept one or more node instances as
  arguments, then assign them to attributes based on subclass-specific
  input_names attribute (same length as expected number of arguments
  to init method). Finally call self.forward() and save result in
  value attribute.
- backward() method should begin by calling self.gradient(), resulting
  in a tuple of gradients (one for each item of input_name). Then save
  each gradient as the grad attribute of the corresponding input
  node. You can include error checking code to ensure that the shapes
  of the value and grad are the same. Finally call backward method of
  each input node, which results in a recursive call to compute
  gradients for all nodes in the computation graph.

** Classes: mm,relu,logistic_loss

- should inherit from (be subclass of) Operation.
- should define input_names attribute as a tuple of names of input
  nodes.
- forward() method should return the value computed by this operation,
  based on values in input nodes. Will be called on instantiation, and
  result saved in value attribute.
- gradient() method should return tuple of gradients, one for each
  input node. Will be called from within backward() method.
- Usage should be as below:

#+begin_src python
  a_node = mm(feature_node, weight_node)
  h_node = relu(a_node)
  loss_node = logistic_loss(a_node, label_node)
  loss_node.backward() #assigns grad attributes.
  print(weight_node.grad) #should be defined by now.
#+end_src

** Class: AutoMLP (automatic multi-layer perceptron)

This should be similar to what we did last week, except that instead
of the compute_gradient method, there should be a take_step method
which uses the new node classes, as described above.

- __init__ method should store hyper-parameters, max_epochs,
  batch_size, step_size, and units_per_layer (list or numpy array of
  positive integers, first element should equal number of input
  features, last element should be 1). For debugging you may want to set units_per_layer = [n_input_features, 1] which means you will get a linear model and batch_size=n_rows (same as linear model homework). Also initialize attribute
  weight_node_list to a list of node instances, each node value should
  be a numpy matrix with random values near zero for one of the
  initial layer prediction functions in the neural network.
- take_step(X=batch_features, y=batch_labels) method should
  - begin by creating node instances for X,y.
  - use a for loop over layers to compute intermediate nodes using
    Operation subclasses. Note that this code is defining both the
    forward and backward propagation via a computation graph, but the
    details of the gradient computations are abstracted away in the
    Operation subclasses.
  - call backward() on the final node instance (mean loss) to compute
    and store gradients in each node. 
  - use a for loop over nodes in weight_node_list to update each
    parameter matrix (take a step in the negative gradient direction).
- fit(X=subtrain_features, y=subtrain_labels) method should run
  gradient descent until max_epochs is reached. There should be two
  for loops, first over epochs, then over batches. You should use the
  take_step method on each batch. Optionally compute the
  subtrain/validation loss at the end of each epoch.
- decision_function(X=test_features) method should return a numpy
  array of real number predicted scores given the current weights in
  the neural network.
- predict(X=test_features) method should return a numpy array of
  predicted classes given the current weights in the neural network.

** Hyper-parameter training and diagnostic plot

You can use either of the two options which were described in the 
homework on linear models.
- AutoMLPCV: a single class with a fit method that splits train into
  subtrain and validation sets, then computes loss with respect to
  both sets at the end of each epoch.
- AutoMLP+MyCV: two different classes, one as described above
  (AutoMLP), and the other (MyCV) which handles hyper-parameter
  selection via subtrain/validation splits, and should have no code
  specific to neural networks.

Whichever method you choose, run it on the full spam/zip data sets,
and make a plot for each data set, of subtrain/validation loss as a
function of number of epochs. For full credit your validation loss
should show the expected U shape (if it does not, then you may need to
change hyper-parameters). According to your plot, what is the best
number of epochs for spam? For zip?

** Experiments/application

- Use similar experimental setup as last homework on linear models
  (with 3-fold CV train/test splits defined by KFold, and with
  GridSearchCV+KNeighborsClassifier and LogisticRegressionCV), but add
  your new algorithm to compare.
- Make sure to run experiments on both spam and zip data. This time
  make sure to scale the data sets before putting them into the
  data_dict (so you don't have to worry about scaling in neural
  network code). Show a table of resulting test accuracy numbers, as
  well as a ggplot like in last homework. On the ggplot y axis there
  should be at least the following algorithms: featureless,
  GridSearchCV+KNeighborsClassifier, LogisticRegressionCV, your new
  algorithm (either AutoMLPCV or AutoMLP+MyCV).
- Does your implementation get similar test accuracy as scikit-learn,
  or better?  (it should!)

** Extra credit

- Show your MyMLP class from last week on your test accuracy plot. Is
  it more accurate than AutoMLP, or about the same? (it should be
  about the same if both were implemented correctly)
- Implement learning an intercept for every hidden/output unit, as an
  instantiation parameter in AutoMLP(intercept=True). Show both
  intercept=True and False on your test accuracy plot: which is more
  accurate, or are they about the same? (it should be about the same,
  maybe a little more accurate with intercept)
  
** FAQ

- How to debug? For debugging you may want to set units_per_layer = [n_input_features, 1] which means you will get a linear model and batch_size=n_rows (same as linear model homework).
