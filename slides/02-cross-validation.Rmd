---
title: "Introduction to supervised machine learning, k-fold cross-validation, nearest neighbors, and linear models"
author: "Toby Dylan Hocking"
output: beamer_presentation
---

```{r opts, echo=FALSE}
knitr::opts_chunk$set(
  echo=FALSE, results=FALSE,
  fig.width=10,
  fig.height=6)
Sys.setenv(RETICULATE_PYTHON=if(.Platform$OS.type=="unix")
  "/home/tdhock/.local/share/r-miniconda/envs/cs570s22/bin/python"
  else "~/Miniconda3/envs/cs570s22/python.exe")
reticulate::use_condaenv("cs570s22", required=TRUE)
in_render <- !is.null(knitr::opts_knit$get('rmarkdown.pandoc.to'))
```

# Supervised machine learning

- Goal is to learn a function $f(\mathbf x)=y$ where $\mathbf
  x$ is an
  input/feature vector and $y$ is an output/label.
- $x=$image of digit/clothing, $y\in\{0,\dots,9\}$ (ten classes).
- $x=$vector of word counts in email, $y\in\{1,0\}$ (spam or not).
- $x=$image of retina, $y=$risk score for heart disease.
- This week we will focus on a specific kind of supervised learning
  problem called binary classification, which means $y\in\{1,0\}$.
  
---

# Learning algorithm

- We want a learning algorithm \textsc{Learn} which inputs a training
  data set and outputs a prediction function $f$.
- In math a training data set with $n$ observations and $p$ features
  is a matrix $\mathbf X\in\mathbb R^{n\times p}$ with a 
  label vector $\mathbf y\in\{0,1\}^n$.
- On computers it is a CSV file with $n$ rows and $p+1$ columns.
- Want: $\textsc{Learn}(\mathbf X, \mathbf y)\rightarrow f$.
- We will use three such data sets from Elements of Statistical
  Learning book by Hastie et al. (mixture slightly modified)
  
```{=latex}
%>>> {k:X.shape for k, (X,y) in data_dict.items()}
%{'spam': (4601, 57), 'zip': (623, 256), 'mixture': (200, 2)}
\small
\begin{tabular}{crrc}
name &observations, $n$ & inputs/features, $p$ & outputs/labels \\
\hline
zip.test & images, 623 & pixel intensities, 256 & 0/1 digits \\
spam & emails, 4601 & word counts, 57 & spam=1/not=0 \\
mixture & people, 200 & height/weight, 2  & democratic/republican \\
\end{tabular}
```

\url{https://github.com/tdhock/cs570-spring-2022/tree/master/data}

\url{https://hastie.su.domains/ElemStatLearn/data.html}

---

# Mixture data table

```{python results=TRUE}
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold
from sklearn.linear_model import LassoCV
import plotnine as p9
import numpy as np
import pandas as pd
from urllib.request import urlretrieve
import os
import webbrowser
# work-around for rendering plots under windows, which hangs within
# emacs python shell: instead write a PNG file and view in browser.
on_windows = os.name == "nt"
in_render = r.in_render if 'r' in dir() else False
using_agg = on_windows and not in_render
if using_agg:
    import matplotlib
    matplotlib.use("agg")
def show(g):
    if not using_agg:
        return g
    g.save("tmp.png")
    webbrowser.open('tmp.png')

data_dict = {}

spam_df = pd.read_csv(
    "~/teaching/cs570-spring-2022/data/spam.data",
    header=None, sep=" ")
nrow, ncol = spam_df.shape
label_col_num = ncol-1
col_num_vec = spam_df.columns.to_numpy()
label_vec = spam_df[label_col_num]
feature_mat = spam_df.iloc[:,col_num_vec != label_col_num]
data_dict["spam"] = (feature_mat, label_vec)

zip_df = pd.read_csv(
    "~/teaching/cs570-spring-2022/data/zip.test.gz",
    sep=" ", header=None)
label_col_num = 0
col_num_vec = zip_df.columns.to_numpy()
all_label_vec = zip_df[label_col_num]
is01 = all_label_vec.isin([0,1])
label_vec = all_label_vec[is01]
feature_mat = zip_df.loc[is01,col_num_vec != label_col_num]
data_dict["zip"] = (feature_mat, label_vec)

mixture_df = pd.read_csv(
    "~/teaching/cs570-spring-2022/data/ESL.mixture.csv")
#mixture_df.query('party == "democratic" & height_in > 70')
label_col_name = "party"
col_name_vec = mixture_df.columns.to_numpy()
party_vec = mixture_df[label_col_name]
party_tuples = [
    ("democratic","blue",0),
    ("republican","red",1)
]
party_colors = {party:color for party,color,number in party_tuples}
party_number_dict = {party:number for party,color,number in party_tuples}
number_party_dict = {number:party for party,color,number in party_tuples}
def number_to_party_vec(v):
    return np.where(v==0, number_party_dict[0], number_party_dict[1])
label_vec = np.where(
    party_vec == "democratic",
    party_number_dict["democratic"],
    party_number_dict["republican"])
feature_mat = mixture_df.loc[:,col_name_vec != label_col_name]
data_dict["mixture"] = (feature_mat, label_vec)
pd.set_option("display.max_columns", 0)
mixture_df
```

---

# Spam data table

```{python results=TRUE}
pd.set_option("display.max_columns", 6)
spam_df
```

---

# Zip.test data table

```{python results=TRUE}
pd.set_option("display.max_columns", 6)
zip_df
```

---

# Visualize mixture data set

- Each axis represents one column of the $\mathbf X$ matrix.
- Each point represents one row of the $\mathbf X$ matrix.
- Color represents class label $\mathbf y$.

```{python out.height='75%'}
gg = p9.ggplot()+\
    p9.theme(subplots_adjust={'right': 0.8})+\
    p9.geom_point(
        p9.aes(
            x="height_in",
            y="weight_lb",
            color="party"
        ),
        data=mixture_df)+\
    p9.scale_color_manual(
        values=party_colors)
show(gg)
```

---

# Function viz

```{python}
neigh = KNeighborsClassifier(n_neighbors=1)
neigh.fit(feature_mat, label_vec)
def make_grid(mat, n_grid = 80):
    assert mat.shape[1] == 2
    mesh_args = mat.apply(
        lambda x: np.linspace(min(x),max(x), n_grid), axis=0)
    mesh_tup = np.meshgrid(*[mesh_args[x] for x in mesh_args])
    mesh_vectors = [v.flatten() for v in mesh_tup]
    return pd.DataFrame(dict(zip(mesh_args,mesh_vectors)))
grid_df = make_grid(feature_mat)
#https://stackoverflow.com/questions/55496700/starred-expression-inside-square-brackets does not work?
#np.ogrid[-1:1:5j, -2:3:0.5]
#slice_vec = feature_mat.apply(lambda x: slice(min(x),max(x), complex(imag=n_grid)), axis=0)
grid_mat = grid_df.to_numpy()
grid_df["prediction"] = neigh.predict(grid_mat)
prob_mat = neigh.predict_proba(grid_mat)
grid_df["prob1"] = prob_mat[:,1]
grid_df["party"] = number_to_party_vec(grid_df.prediction)
gg = p9.ggplot()+\
    p9.theme_bw()+\
    p9.theme(subplots_adjust={'right': 0.8})+\
    p9.geom_point(
        p9.aes(
            x="height_in",
            y="weight_lb",
            color="party"
        ),
        size=0.1,
        data=grid_df)+\
    p9.geom_point(
        p9.aes(
            x="height_in",
            y="weight_lb",
            fill="party"
        ),
        color="black",
        size=2,
        data=mixture_df)+\
    p9.scale_color_manual(
        values=party_colors)+\
    p9.scale_fill_manual(
        values=party_colors)
show(gg)

```

---

# Predicted decision bounary in black

```{python}
# https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.find_contours
# https://scikit-image.org/docs/dev/auto_examples/edges/plot_contours.html#sphx-glr-auto-examples-edges-plot-contours-py
from skimage import measure
prob1_mat = grid_df.prob1.to_numpy().reshape([n_grid,n_grid]).transpose()
contours = measure.find_contours(prob1_mat, 0.5)
contour_df_list = []
half_df = (mesh_args-mesh_args.diff()/2)[1:]
half_df.index = [x-0.5 for x in half_df.index]
lookup_df = pd.concat([mesh_args, half_df])
for contour_i, contour_mat in enumerate(contours):
    one_contour_df = pd.DataFrame(contour_mat)
    one_contour_df.columns = [c+"_i" for c in mesh_args]
    one_contour_df["contour_i"] = contour_i
    for cname in lookup_df:
        iname = cname+"_i"
        contour_col = one_contour_df[iname]
        lookup_col = lookup_df[cname]
        index_df = lookup_col[contour_col].reset_index()
        one_contour_df[cname] = index_df[cname]
    contour_df_list.append(one_contour_df)
contour_df = pd.concat(contour_df_list)
gg_contour = gg+\
    p9.geom_path(
        p9.aes(
            x="height_in",
            y="weight_lb",
            group="contour_i",
        ),
        data=contour_df)
show(gg_contour)
```

---

# TODO CV figure

- All observations are classified correctly, is this good?

---

# Basic idea of nearest neighbors

---

# Basic idea of linear model

---

# Visualize iris data without labels

- Let $X\in\mathbb R^{150\times 2}$ be the data matrix (input for clustering).

